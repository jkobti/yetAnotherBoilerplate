# Kubernetes Platform Guide

This companion to `Docs/main.md` captures the Kubernetes-first operational model for the yetanotherboilerplate project. Use it when evolving infrastructure automation, onboarding new services, or adjusting cluster-level workflows.

## 1. Activation & Deactivation Strategy

Provide consistent, low-effort ways to enable or disable components across environments.

### 1.1 Helm values (recommended)

- Drive component toggles through chart values, e.g.:

	values.yaml

	```yaml
	mobile:
		enabled: false
	workers:
		enabled: true
	```

- Guard template rendering with `{{- if .Values.<component>.enabled }}` checks.
- Backend feature specifics (Celery workers, transactional email, WebSockets, etc.) live in `Docs/backend-api.md`.

### 1.2 Optional Components (Redis & Workers)

The project supports optional deployment of Redis and Celery workers for background task processing.

**Redis:**
- Chart: `charts/redis`
- Deploy: `make deploy-redis`
- Purpose: Message broker for Celery and caching backend.

**Celery Worker:**
- Configuration: `charts/api/values.yaml` (under `worker`)
- Deploy: `make deploy-worker` (sets `worker.enabled=true`)
- Purpose: Consumes tasks from Redis and executes them asynchronously.

### 1.3 Kustomize overlays

- Maintain a base manifest set and use overlays for `production`, `staging`, and `local` that add or strip resources.

### 1.4 Environment variables / runtime flags

- Gate feature-level changes (not infra) with env vars read by the service (e.g., `FEATURE_X_ENABLED=true`).

### 1.5 Build-time toggles (mono-repo packages)

- Include or exclude packages from build/test matrices via CI configuration or workspace settings.

### 1.6 Feature flag systems

- For runtime gating across platforms, integrate a feature flag service and default to safe OFF values until explicitly enabled.

## 2. Repository Structure (reference)

This is the opinionated monorepo layout used throughout the docs. Adapt to multi-repo if preferred.

```
yetanotherboilerplate/
├── README.md
├── charts/                  # Helm charts for each deployable component
├── k8s/                     # kustomize base + overlays
├── packages/
│   ├── web/                 # frontend web app (React/Vite)
│   ├── ios/                 # iOS app (Xcode project)
│   ├── android/             # Android app
│   ├── api/                 # backend API
│   └── worker/              # background worker
├── infra/                   # terraform / cloud infra (optional)
├── .github/workflows/       # example CI pipeline definitions
└── docs/                    # extended docs, runbooks, checklists
```

Notes:
- `charts/` contains Helm charts that accept `enabled` flags so you can deploy a subset of components.
- `k8s/overlays/local` should stay developer-friendly (e.g., kind or minikube + skaffold/tilt support).

## 3. Deployment Patterns

### 3.1 Helm toggles example

- To deploy only API and web components, set values like:

	```yaml
	api:
		enabled: true
	web:
		enabled: true
	mobile:
		enabled: false
	worker:
		enabled: false
	```

- Apply with `helm upgrade --install myrelease ./charts -f production-values.yaml`.

### 3.2 Kustomize overlay example

- Base contains `deployment-web.yaml`, `deployment-api.yaml`, and `deployment-worker.yaml`.
- Overlay `local` can remove `worker` via a `kustomization.yaml` that excludes or patches it out.

### 3.3 Admin Portal (Webapp)

- Deploy the Admin Portal as a separate web application with its own Deployment/Service and Ingress.
- Recommended routing:
	- Separate host: `admin.yourdomain.com` → isolates cookies, CSP, and security headers.
	- Or reserved path: `/admin` on the primary host with stricter cache and header policies.
- Chart toggles (example):

	```yaml
	admin:
		enabled: true
		host: admin.yourdomain.com
		path: /
	```

- Restrict access server-side via authentication and RBAC; optionally add ingress-level protections (IP allowlists, identity-aware proxy) per environment.
- Admin-only API endpoints should live under a distinct URL namespace (e.g., `/admin/...`) to simplify routing and firewall rules.

## 4. Developer Workflows

- **Local dev (fast iteration):** run backend locally (node/python) against local Postgres, start the web app with dev tooling (e.g., Vite), and rely on kind or minikube plus skaffold/tilt for Kubernetes parity.
- **Full cluster testing:** use kind + Helm to deploy overlays matching staging before merging.
- **Mobile development:** use local simulators with a locally running API (tunnel via ngrok or expose through a local ingress).

## 5. CI/CD Workflow

- **Pipeline stages:**
	1. Lint, static analysis, unit tests
	2. Build artifacts (Docker images, mobile builds), tag
	3. Integration tests on an ephemeral cluster (optional but recommended)
	4. Publish images to the registry
	5. Deploy via Helm using chart-value toggles
- Prefer ephemeral environments for pull requests; tear them down automatically after merge.

## 6. Observability & Security

- Ship optional charts for Prometheus, Grafana, and OpenTelemetry, controlled via Helm values.
- Configure Sentry in frontend builds through environment variables (DSNs) supplied at build time.
- Use cert-manager to manage TLS for ingress resources and issue mutual-auth certificates when services authenticate directly (e.g., frontend → backend mTLS).
- Store secrets in external systems (Vault, cloud secret managers) or use sealed-secrets for GitOps-friendly workflows.

## 7. Component Onboarding Checklist

1. Add the component folder under `packages/` with build/test scripts.
2. Create Helm charts or Kustomize manifests in `charts/` with an `enabled` value.
3. Update CI to build/test the component when toggled on or when its files change.
4. Document runtime env vars and configuration schema under `docs/`.
5. Define a scaling strategy for the component and document it:
	- Resource requests/limits (baseline sizing and target utilization)
	- Vertical vs. horizontal scaling plan (when to scale up vs. out)
	- HPA/VPA policies and metrics (CPU/memory/custom), min/max replicas
	- PodDisruptionBudget, Pod anti-affinity, topology spread constraints
	- Any stateful constraints or external managed dependencies (e.g., DB/Redis)

## 8. Validation & Quality Gates

- All unit tests and linters must pass.
- Run `helm template` with validators like `kubeval` or `conftest`.
- Optionally execute integration smoke tests against a disposable cluster before promotion.

## 9. Edge Cases & Gotchas

- Provide sensible defaults for empty or null configuration values in Helm charts.
- Use selective CI (path filters, test matrices) to keep monorepo build times manageable.
- Fail fast when required secrets are missing in production; ensure logs surface the missing key name.
- Version backend APIs to preserve mobile compatibility; coordinate releases across platforms.
- Database initialization ordering: when deploying a new Postgres instance and API together, use an init container to wait for the database readiness (e.g. `pg_isready`) before running `python manage.py migrate`. Override the Postgres chart `fullname` to a stable DNS name (`postgres`) so `postgres:5432` resolves consistently; otherwise the generated Service name may include the release prefix (e.g. `postgres-postgres`).

## 10. Next-Step Ideas

1. Iterate the component list to reflect the default deployment profile.
2. Add concrete Helm chart skeletons and Kustomize bases as templates.
3. Document CI pipeline examples (e.g., GitHub Actions) alongside a kind/tilt local workflow.
4. (Optional) Scaffold a minimal API + web sample to validate the end-to-end deployment path.

## 11. Networking & Routing

Ingress controllers manage external traffic for the cluster. Key considerations:

- **Multi-domain routing:** map distinct hosts such as `app.yourdomain.com` (web) and `api.yourdomain.com` (backend API).
- Add an admin host such as `admin.yourdomain.com` (Admin Portal webapp) or a reserved path `/admin` on the primary host.
- **Controller choice:** NGINX remains a robust default; alternatives like Traefik or Istio ingress gateways fit particular needs.
- **TLS termination:** terminate TLS at the ingress via cert-manager issued certificates.

Document any additional ingress requirements (mTLS, custom headers, WAF integration) in environment-specific overlays.

---

Treat this guide as living documentation. When introducing new cluster services or deployment patterns, update this file and reference it from `Docs/main.md`.

## 12. Scaling & Capacity Planning

Establish explicit scale up/out plans for each deployable app (API, worker, web, admin, etc.). Capture the plan in the component's chart values and docs.

### 12.1 Principles

- Start with right-sized resource requests/limits and a clear target utilization (e.g., 60–70%).
- Prefer scale out (horizontal) for stateless services; use scale up (vertical) sparingly to remove headroom bottlenecks.
- Autoscale using SLO-aligned signals, not just CPU. Consider latency, error rate, RPS, queue depth, or custom metrics.
- Bound autoscaling with sensible min/max replicas and budget for warm-up time and cold starts.
- For stateful systems (databases, caches, queues), prefer managed services; if self-hosted, plan StatefulSet scaling, storage, and failover explicitly.

### 12.2 Implementation checklist per component

- Requests/limits: baseline CPU/memory and target utilization
- HPA: metric(s), behavior (scale up/down stabilization windows), min/max replicas
- VPA: Off (recommendation-only) by default; enable updates only when safe; coordinate with HPA
- PDB: maintain availability during node upgrades/evictions
- Topology: anti-affinity or topology spread to avoid single-node concentration
- Readiness/liveness probes: ensure autoscaler only counts healthy pods
- Rollout: configure surge/unavailable to maintain capacity during deploys
- Runbooks: load test steps and autoscaling verification in staging

### 12.3 Common patterns

- API Deployments: HPA on CPU + requests-per-second or P90 latency; start with 2–3 replicas, max based on budget
- Background workers: HPA on queue depth (e.g., messages per consumer) with aggressive scale out; ensure idempotency
- Web (static/SSR): static served via CDN (scale at edge); SSR via HPA on concurrency/latency
- Scheduled jobs/CronJobs: set `concurrencyPolicy` and resource caps to avoid contention
- Cluster autoscaler: ensure node pools can grow to support max replicas; use appropriate instance sizes and taints/tolerations for specialized workloads

Document the chosen strategy in the component's README and Helm values along with alerts that indicate when limits are reached and a re-size is required.
